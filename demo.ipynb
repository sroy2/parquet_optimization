{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents:\n",
    "\n",
    "## [Recap](#start)\n",
    "\n",
    "## [Problem 1](#p1)\n",
    "\n",
    "## [Problem 2](#p2)\n",
    "\n",
    "## [Problem 3](#p3)\n",
    "\n",
    "## [Advanced Parquet Inspection](#advanced-parquet)\n",
    "\n",
    "## [Just Give Me Code Without Explaining It!](#end)\n",
    "\n",
    "### Disclaimer: I am not a TA/Grader\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"start\"></a>\n",
    "# Recap on Storage, ROW vs COLUMN\n",
    "\n",
    "<img src=\"img/1.png\"/>\n",
    "\n",
    "<img src=\"img/2.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# So what is parquet?\n",
    "\n",
    "<img src=\"img/3.png\"/>\n",
    "\n",
    "<img src=\"img/4.png\"/>\n",
    "\n",
    "<img src=\"img/5.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Great, so about that lab...\n",
    "\n",
    "## How do I get into dumbo again?\n",
    "<img src=\"img/6.png\" width=\"400\"/>\n",
    "\n",
    "## Open terminal/bash/powershell... etc\n",
    "```sh\n",
    "ssh your_netid@gw.hpc.nyu.edu\n",
    "ssh dumbo.hpc.nyu.edu\n",
    "```\n",
    "\n",
    "## Onto the lab... what. How do I clone the repo? NotLikeThis\n",
    "\n",
    "1. Go to the [NYU-Big-Data](https://github.com/nyu-big-data) Git repo...\n",
    "2. Click on the Lab4-Storage link.\n",
    "3. Click the GREEN button named \"Clone\" to get your repo link:\n",
    "<img src=\"img/7.png\"/>\n",
    "4. Go back to your dumbo window...\n",
    "5. clone the repo:\n",
    "```sh\n",
    "git clone your_copied_repo_link\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"p1\"></a>\n",
    "## Part 1: Benchmarking queries\n",
    "\n",
    "If you can't write a query by now just quit the course. \n",
    "Seriously... save me the time and google the compute cycles.\n",
    "\n",
    "```sh\n",
    "+----------+---------+-------+-------+                                          \n",
    "|first_name|last_name| income|zipcode|\n",
    "+----------+---------+-------+-------+\n",
    "|   Annette|   Abbott|72870.0|  14763|\n",
    "|    Hailey|   Abbott|72182.0|  75097|\n",
    "|   Jocelyn|   Abbott|56574.0|   3062|\n",
    "|     Sheri|   Abbott|64952.0|  77663|\n",
    "|     Sonya|   Abbott|86156.0|  79072|\n",
    "+----------+---------+-------+-------+\n",
    "```\n",
    "\n",
    "<div class=\"alert alert-danger\" role=\"alert\">\n",
    "  <strong>(Problem 1a)</strong> You need to write these queries:\n",
    "</div>\n",
    "\n",
    "1. csv_avg_income: the `average income` grouped by `zipcode`\n",
    "2. csv_max_income: the `maximum income` grouped by `last_name`\n",
    "3. csv_anna: people with `first_name` of 'Anna' and `income` at least 70000\n",
    "\n",
    "### I can write SQL... but where do I put it?\n",
    "\n",
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "  <strong>(Hint)</strong> Open queries.py\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm pseudo code example similar to what is found in queries.py\n",
    "#\n",
    "# --- ADD YOUR NEW QUERIES BELOW ---\n",
    "def fake_query(spark, file_path):\n",
    "    ## the query_name starts with CSV\n",
    "    df = spark.read.csv(file_path, header=True, \n",
    "               schema='first_name STRING, last_name STRING, income FLOAT, zipcode INT')\n",
    "    ## the query_name starts with PQ\n",
    "    df = spark.read.parquet(file_path)\n",
    "    df.createOrReplaceTempView('people')\n",
    "    \n",
    "    query = \"I'm the SQL query you meant to write, hear me roar!\"\n",
    "    fake_query = spark.sql(query)\n",
    "    return fake_query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\" role=\"alert\">\n",
    "  Notice that, when reading a csv we need to either specify or infer a schema. This is because csv's are only semi-structured data. Just like relational DBs, parquet files are structured data - so spark is told expressly what to expect for each column when you read a parquet file.\n",
    "</div>\n",
    "\n",
    "### Awesome! But they want me to run 25 queries and save the output and stuff... \n",
    "\n",
    "I see you don't want to do any work. I've added a [csv_table helper function](https://github.com/sroy2/parquet_optimization/blob/master/bench.py#L102-L167) to my bench.py you can use.\n",
    "\n",
    "\n",
    "### Great, I copied the code - how do I call it?\n",
    "\n",
    "Aha! This is a good question!\n",
    "1. On dumbo go into the lab4 folder you cloned.\n",
    "2. load the following modules to tell dumbo what python and spark versions/environments we want to use:\n",
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "  <strong>(Hint)</strong> This is important if you want the right version of pyspark! I got lazy of typing them so I put them into my .bashrc (a config file that runs every time you log in).\n",
    "</div>\n",
    "\n",
    "```sh\n",
    "module load python/gnu/3.6.5\n",
    "module load spark/2.4.0\n",
    "pyspark\n",
    "```\n",
    "\n",
    "Now wait for pyspark to finish loading.\n",
    "\n",
    "<div class=\"alert alert-warning\" role=\"alert\">\n",
    "    You may wonder what all of those <strong>WARN Utils</strong> messages are about. The port messages are letting you know that the driver-node (dumbo in this case) can't bind or assign that port (<a href=\"https://spark.apache.org/docs/latest/monitoring.html\">starting at 4040</a>) for communication with the cluster manager. This is probably because someone else is running a spark instance that already has it bound.<br>\n",
    "</div>\n",
    "\n",
    "<img src=\"img/8.png\"/>\n",
    "\n",
    "<div class=\"alert alert-warning\" role=\"alert\">\n",
    "    If you get <strong>WARN HiveConf</strong> (server misconfiguration) or <strong>WARN Client</strong> (you might be missing a declartion in your ~/.bashrc)... Well it probably doesn't matter. Ask on the <a href=\"https://groups.google.com/a/nyu.edu/forum/#!forum/ds-ga-1004-SP20-d65b\">Google Group</a> if your code doesn't run because of it. But... those warnings <strong>probably aren't</strong> why your code breaks.\n",
    "</div>\n",
    "\n",
    "```sh\n",
    "-bash-4.1$ more ~/.bashrc \n",
    "HADOOP_EXE='/usr/bin/hadoop'\n",
    "HADOOP_LIBPATH='/opt/cloudera/parcels/CDH/lib'\n",
    "HADOOP_STREAMING='hadoop-mapreduce/hadoop-streaming.jar'\n",
    "\n",
    "alias hfs=\"$HADOOP_EXE fs\"\n",
    "alias hjs=\"$HADOOP_EXE jar $HADOOP_LIBPATH/$HADOOP_STREAMING\"\n",
    "\n",
    "module load python/gnu/3.6.5\n",
    "module load spark/2.4.0\n",
    "-bash-4.1$ pyspark\n",
    "Welcome to\n",
    "      ____              __\n",
    "     / __/__  ___ _____/ /__\n",
    "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
    "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.4.0\n",
    "      /_/\n",
    "\n",
    "Using Python version 3.6.5 (default, May  3 2018 09:34:46)\n",
    "SparkSession available as 'spark'.\n",
    "```\n",
    "\n",
    "### Great I'm in - now what?\n",
    "\n",
    "I'm just going to assume you wrote the queries above already. <br>\n",
    "\n",
    "Time to import our modules so we can call them from our spark session and read in the data!\n",
    "\n",
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "  <strong>(Hint)</strong> Don't forget to import modules now that you're in pyspark!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "import queries\n",
    "import bench\n",
    "\n",
    "queries.csv_anna(spark, 'hdfs:/user/bm106/pub/people_small.csv')\n",
    "_.show()\n",
    "\n",
    "bench.benchmark(spark, 25, queries.csv_anna, 'hdfs:/user/bm106/pub/people_small.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    ">>> _.show()\n",
    "+----------+---------+-------+                                                  \n",
    "|first_name|last_name| income|\n",
    "+----------+---------+-------+\n",
    "|      Anna|   Bailey|72458.0|\n",
    "|      Anna| Mcknight|74660.0|\n",
    "|      Anna|  Ferrell|74849.0|\n",
    "|      Anna|Velasquez|74693.0|\n",
    "+----------+---------+-------+\n",
    "\n",
    ">>> bench.benchmark(spark, 25, q.csv_anna, 'hdfs:/user/bm106/pub/people_small.csv')\n",
    "[0.0844266414642334, 0.08226633071899414, 0.08829736709594727, 0.0897524356842041, 0.07845854759216309, 0.07774877548217773, 0.07760429382324219, 0.0960390567779541, 0.0782325267791748, 0.07440423965454102, 0.07535243034362793, 0.08235287666320801, 0.07107186317443848, 0.07475018501281738, 0.07666397094726562, 0.07802414894104004, 0.07164120674133301, 0.07479310035705566, 0.0723581314086914, 0.08651256561279297, 0.07664608955383301, 0.07851982116699219, 0.07354259490966797, 0.07335448265075684, 0.08263015747070312]\n",
    "```\n",
    "\n",
    "<div class=\"alert alert-warning\" role=\"alert\">\n",
    "    <i>Woaaah, what is that black-magic underscore thingy you did?</i> <br> <br>\n",
    "    In python an underscore is a placeholder variable that stores the output of the last statement you executed. In this case it's holding an unexecuted dataframe object. I execute that object by calling an action, in this case .show(), which triggers the spark operation.\n",
    "</div>\n",
    "\n",
    "### Awesome so your imported query works!\n",
    "\n",
    "*You said something about a helper function?*\n",
    "\n",
    "1. Ok ya bum, I'm assuming you're writing your report in a jupyter notebook.\n",
    "2. We will use pandas for easy data import and table formatting (so import it!)\n",
    "3. You pass the entire line we just ran into the helper function as a string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "test = \"bench.benchmark(spark, 25, queries.csv_anna, 'hdfs:/user/bm106/pub/people_small.csv')\"\n",
    "bench.csv_table(spark, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    ">>> bench.csv_table(spark, test)\n",
    "query,file,trials,minimum,median,maximum\n",
    "csv_anna,people_small,25,0.075,0.092,0.115\n",
    "```\n",
    "\n",
    "*Great! But... can't we run it over all the files?*\n",
    "\n",
    "1. Yes... I anticipated your needs.\n",
    "2. The helper function can take a third parameter to batch process tasks:\n",
    "\n",
    "```python\n",
    "    runtype : string | None, 'single', 'all', 'csv', 'pq'\n",
    "    \n",
    "        Optional parameter, if not set only benchmark_cmd results are printed.\n",
    "        \n",
    "        'all'    run all pq|csv queries on all file sizes\n",
    "        'csv'    runs csv queries with all bm106 csv filepath sizes\n",
    "        'pq'     runs pq queries with all provided pq filepath sizes\n",
    "        'both'   runs both, csv and pq queries as stated above\n",
    "```\n",
    "\n",
    "3. Let's try again, but this time with 'csv':\n",
    "\n",
    "<div class=\"alert alert-warning\" role=\"alert\">\n",
    "    <strong>(Warning)</strong> You are getting ready to loop over multiple files and/or queries multiple (hopefully less than 26) times! Jobs can go really slow - espcially csv jobs. The progress bar is your friend. If you can't wait or think something broke, <strong>CTRL+C</strong> should break out. In the mean time get a coffee, watch a video, or take a 5 minute nap.   : )\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "bench.csv_table(spark, test, 'csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    ">>> bench.csv_table(spark, test, 'csv')\n",
    "query,file,trials,minimum,median,maximum                                        \n",
    "csv_anna,people_small,25,0.066,0.075,0.164\n",
    "csv_anna,people_medium,25,0.562,0.579,0.731\n",
    "csv_anna,people_large,25,29.017,29.900,32.316\n",
    "```\n",
    "\n",
    "*Well look at that - but you didn't output to my jupyter notebook...*\n",
    "\n",
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "  <strong>(Hint)</strong> Just copy it!\n",
    "</div>\n",
    "\n",
    "<img src=\"img/9.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>file</th>\n",
       "      <th>trials</th>\n",
       "      <th>minimum</th>\n",
       "      <th>median</th>\n",
       "      <th>maximum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>csv_anna</td>\n",
       "      <td>people_small</td>\n",
       "      <td>25</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>csv_anna</td>\n",
       "      <td>people_medium</td>\n",
       "      <td>25</td>\n",
       "      <td>0.562</td>\n",
       "      <td>0.579</td>\n",
       "      <td>0.731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>csv_anna</td>\n",
       "      <td>people_large</td>\n",
       "      <td>25</td>\n",
       "      <td>29.017</td>\n",
       "      <td>29.900</td>\n",
       "      <td>32.316</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      query           file  trials  minimum  median  \\\n",
       "0  csv_anna   people_small      25    0.066   0.075   \n",
       "1  csv_anna  people_medium      25    0.562   0.579   \n",
       "2  csv_anna   people_large      25   29.017  29.900   \n",
       "\n",
       "   maximum                                          \n",
       "0                                            0.164  \n",
       "1                                            0.731  \n",
       "2                                           32.316  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_1 = pd.read_clipboard(sep=\",\")\n",
    "df_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Great! But... why didn't you just run all the queries at once?*\n",
    "\n",
    "1. We could have, read the options for your helper function again.\n",
    "```python\n",
    ">>> bench.csv_table(spark, test, 'all csv')\n",
    "query,file,trials,minimum,median,maximum                                        \n",
    "csv_avg_income,people_small,25,1.299,1.790,2.282\n",
    "csv_avg_income,people_medium,25,2.050,2.281,2.892\n",
    "csv_avg_income,people_large,25,29.432,30.107,32.163\n",
    "csv_max_income,people_small,25,1.241,1.544,1.884\n",
    "csv_max_income,people_medium,25,1.871,2.135,5.779\n",
    "csv_max_income,people_large,25,30.263,37.220,42.168\n",
    "csv_anna,people_small,25,0.060,0.085,0.218\n",
    "csv_anna,people_medium,25,0.688,0.740,1.246\n",
    "csv_anna,people_large,25,34.957,39.114,43.670\n",
    "```\n",
    "\n",
    "2. Remember, the more things you chain together the longer it takes to process.\n",
    "\n",
    "<div class=\"alert alert-danger\" role=\"alert\">\n",
    "  <strong>(Problem 1b)</strong> You need to record your results:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "bench.csv_table(spark, test, 'all csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>file</th>\n",
       "      <th>trials</th>\n",
       "      <th>minimum</th>\n",
       "      <th>median</th>\n",
       "      <th>maximum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>csv_avg_income</td>\n",
       "      <td>people_small</td>\n",
       "      <td>25</td>\n",
       "      <td>1.299</td>\n",
       "      <td>1.790</td>\n",
       "      <td>2.282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>csv_avg_income</td>\n",
       "      <td>people_medium</td>\n",
       "      <td>25</td>\n",
       "      <td>2.050</td>\n",
       "      <td>2.281</td>\n",
       "      <td>2.892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>csv_avg_income</td>\n",
       "      <td>people_large</td>\n",
       "      <td>25</td>\n",
       "      <td>29.432</td>\n",
       "      <td>30.107</td>\n",
       "      <td>32.163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>csv_max_income</td>\n",
       "      <td>people_small</td>\n",
       "      <td>25</td>\n",
       "      <td>1.241</td>\n",
       "      <td>1.544</td>\n",
       "      <td>1.884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>csv_max_income</td>\n",
       "      <td>people_medium</td>\n",
       "      <td>25</td>\n",
       "      <td>1.871</td>\n",
       "      <td>2.135</td>\n",
       "      <td>5.779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>csv_max_income</td>\n",
       "      <td>people_large</td>\n",
       "      <td>25</td>\n",
       "      <td>30.263</td>\n",
       "      <td>37.220</td>\n",
       "      <td>42.168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>csv_anna</td>\n",
       "      <td>people_small</td>\n",
       "      <td>25</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>csv_anna</td>\n",
       "      <td>people_medium</td>\n",
       "      <td>25</td>\n",
       "      <td>0.688</td>\n",
       "      <td>0.740</td>\n",
       "      <td>1.246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>csv_anna</td>\n",
       "      <td>people_large</td>\n",
       "      <td>25</td>\n",
       "      <td>34.957</td>\n",
       "      <td>39.114</td>\n",
       "      <td>43.670</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            query           file  trials  minimum  median  \\\n",
       "0  csv_avg_income   people_small      25    1.299   1.790   \n",
       "1  csv_avg_income  people_medium      25    2.050   2.281   \n",
       "2  csv_avg_income   people_large      25   29.432  30.107   \n",
       "3  csv_max_income   people_small      25    1.241   1.544   \n",
       "4  csv_max_income  people_medium      25    1.871   2.135   \n",
       "5  csv_max_income   people_large      25   30.263  37.220   \n",
       "6        csv_anna   people_small      25    0.060   0.085   \n",
       "7        csv_anna  people_medium      25    0.688   0.740   \n",
       "8        csv_anna   people_large      25   34.957  39.114   \n",
       "\n",
       "   maximum                                          \n",
       "0                                            2.282  \n",
       "1                                            2.892  \n",
       "2                                           32.163  \n",
       "3                                            1.884  \n",
       "4                                            5.779  \n",
       "5                                           42.168  \n",
       "6                                            0.218  \n",
       "7                                            1.246  \n",
       "8                                           43.670  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2 = pd.read_clipboard(sep=\",\")\n",
    "df_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"p2\"></a>\n",
    "## Part 2: CSV vs Parquet\n",
    "\n",
    "There are two steps here. First copy some files. Second copy and paste your SQL queries.\n",
    "\n",
    "### How do I copy files?\n",
    "\n",
    "Ok, this isn't quite as trivial as it sounds. We are actually converting the csv files from before into parquet files. This is going from semi-structured data to structured data. The easiest way is using pyspark (and the [copy helper function](https://github.com/sroy2/parquet_optimization/blob/master/bench.py#L86-L100) in bench.py).\n",
    "\n",
    "```python\n",
    "bench.copy(spark)\n",
    "```\n",
    "\n",
    "Yep, it's that easy. So is copying your queries - that just takes more pasting.\n",
    "\n",
    "<div class=\"alert alert-warning\" role=\"alert\">\n",
    "    <strong>(Warning)</strong> Don't forget to change the spark.read.<strong>csv</strong> to spark.read.<strong>parquet</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "bench.copy(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Rinse and repeat.\n",
    "\n",
    "<div class=\"alert alert-danger\" role=\"alert\">\n",
    "  <strong>(Problem 2)</strong> Record the parquet results and compare to the csv results:\n",
    "</div>\n",
    "\n",
    "```python\n",
    ">>> test = \"bench.benchmark(spark, 25, queries.pq_anna, 'hdfs:/user/\"+bench.whoami()+\"/people_small.parquet')\"\n",
    ">>> bench.csv_table(spark, test, 'all pq')\n",
    "query,file,trials,minimum,median,maximum                                        \n",
    "pq_avg_income,people_small,25,1.925,2.578,3.366\n",
    "pq_avg_income,people_medium,25,2.392,3.216,5.029\n",
    "pq_avg_income,people_large,25,1.983,2.395,3.569\n",
    "pq_max_income,people_small,25,1.969,2.374,3.312\n",
    "pq_max_income,people_medium,25,2.018,2.505,3.676\n",
    "pq_max_income,people_large,25,1.993,2.604,3.960\n",
    "pq_anna,people_small,25,0.076,0.081,0.197\n",
    "pq_anna,people_medium,25,0.137,0.153,0.197\n",
    "pq_anna,people_large,25,0.084,0.089,0.124\n",
    "```\n",
    "\n",
    "<div class=\"alert alert-warning\" role=\"alert\">\n",
    "    <strong>(Warning)</strong> You could also run something like 'all both' if you want the both csv and pq results in one table. Keep in mind searching \"large\" csv files takes <strong>significantly more time</strong> than searching \"large\" pq files.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "test = \"bench.benchmark(spark, 25, queries.pq_anna, 'hdfs:/user/\"+bench.whoami()+\"/people_small.parquet')\"\n",
    "bench.csv_table(spark, test, 'all pq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>file</th>\n",
       "      <th>trials</th>\n",
       "      <th>minimum</th>\n",
       "      <th>median</th>\n",
       "      <th>maximum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pq_avg_income</td>\n",
       "      <td>people_small</td>\n",
       "      <td>25</td>\n",
       "      <td>1.925</td>\n",
       "      <td>2.578</td>\n",
       "      <td>3.366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pq_avg_income</td>\n",
       "      <td>people_medium</td>\n",
       "      <td>25</td>\n",
       "      <td>2.392</td>\n",
       "      <td>3.216</td>\n",
       "      <td>5.029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pq_avg_income</td>\n",
       "      <td>people_large</td>\n",
       "      <td>25</td>\n",
       "      <td>1.983</td>\n",
       "      <td>2.395</td>\n",
       "      <td>3.569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pq_max_income</td>\n",
       "      <td>people_small</td>\n",
       "      <td>25</td>\n",
       "      <td>1.969</td>\n",
       "      <td>2.374</td>\n",
       "      <td>3.312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pq_max_income</td>\n",
       "      <td>people_medium</td>\n",
       "      <td>25</td>\n",
       "      <td>2.018</td>\n",
       "      <td>2.505</td>\n",
       "      <td>3.676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>pq_max_income</td>\n",
       "      <td>people_large</td>\n",
       "      <td>25</td>\n",
       "      <td>1.993</td>\n",
       "      <td>2.604</td>\n",
       "      <td>3.960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>pq_anna</td>\n",
       "      <td>people_small</td>\n",
       "      <td>25</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>pq_anna</td>\n",
       "      <td>people_medium</td>\n",
       "      <td>25</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.153</td>\n",
       "      <td>0.197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>pq_anna</td>\n",
       "      <td>people_large</td>\n",
       "      <td>25</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.124</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           query           file  trials  minimum  median  \\\n",
       "0  pq_avg_income   people_small      25    1.925   2.578   \n",
       "1  pq_avg_income  people_medium      25    2.392   3.216   \n",
       "2  pq_avg_income   people_large      25    1.983   2.395   \n",
       "3  pq_max_income   people_small      25    1.969   2.374   \n",
       "4  pq_max_income  people_medium      25    2.018   2.505   \n",
       "5  pq_max_income   people_large      25    1.993   2.604   \n",
       "6        pq_anna   people_small      25    0.076   0.081   \n",
       "7        pq_anna  people_medium      25    0.137   0.153   \n",
       "8        pq_anna   people_large      25    0.084   0.089   \n",
       "\n",
       "   maximum                                          \n",
       "0                                            3.366  \n",
       "1                                            5.029  \n",
       "2                                            3.569  \n",
       "3                                            3.312  \n",
       "4                                            3.676  \n",
       "5                                            3.960  \n",
       "6                                            0.197  \n",
       "7                                            0.197  \n",
       "8                                            0.124  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_3 = pd.read_clipboard(sep=\",\")\n",
    "df_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"p3\"></a>\n",
    "## Part 3: Optimizing Parquet\n",
    "\n",
    "Ok. The part you were waiting for!\n",
    "\n",
    "<div class=\"alert alert-danger\" role=\"alert\">\n",
    "  <strong>(Problem 3)</strong> Try at least three different ways of optimizing parquet files and search for the best configurations. Record the optimized results and compare to the base results above.\n",
    "</div>\n",
    "\n",
    "1. This is not a comprehensive parquet optimization guide... just enough to think about how to get through this homework.\n",
    "2. If you don't care about learning the stuff... just look for the green blocks.\n",
    "3. You still need to do a write up at the end!\n",
    "\n",
    "<div class=\"alert alert-warning\" role=\"alert\">\n",
    "    <strong>(Warning)</strong> You aren't allowed to change any of the query code that you wrote in the previous step. So don't try it!\n",
    "</div>\n",
    "\n",
    "So how do we speed queries up?\n",
    "\n",
    "<img src=\"img/10.png\"/>\n",
    "\n",
    "## Sorting - [documentation](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?#pyspark.sql.DataFrame.sort)\n",
    "\n",
    "Sorting is useful because it helps organize your columns before they get broken up into row groups. The footer information at the bottom of each page gives basic statistics (like min or max values) that can eliminate entire pages from being read. \n",
    "\n",
    "```python\n",
    "# Read\n",
    "file = 'hdfs:/user/'+bench.whoami()+'/people_small'\n",
    "df = spark.read.parquet(file+'.parquet')\n",
    "\n",
    "# Sort Examples:\n",
    "df.sort('zipcode')\n",
    "df.sort('zipcode', 'income')\n",
    "\n",
    "# Write\n",
    "df.write.parquet(file+'_sort.parquet')\n",
    "```\n",
    "\n",
    "But wait you ask - if it's still in one big parquet file, how does that make it any faster? Well... by itself, it might not.\n",
    "\n",
    "*But what do I sort?*\n",
    "\n",
    "<div class=\"alert alert-warning\" role=\"alert\">\n",
    "    You should probably have an idea of what to sort just by reading your query. When it isn't obvious you can call the .explain() on your dataframe. Note - the physical plan from .explain() is read from the bottom up.\n",
    "</div>\n",
    "\n",
    "```python\n",
    ">>> file = \"hdfs:/user/\"+bench.whoami()+\"/people_small.parquet\"\n",
    ">>> queries.pq_anna(spark, file).explain()\n",
    "== Physical Plan ==\n",
    "*(1) Project [first_name#8537, last_name#8538, income#8539]\n",
    "+- *(1) Filter (((isnotnull(first_name#8537) && isnotnull(income#8539)) && (first_name#8537 = Anna)) && (income#8539 >= 70000.0))\n",
    "   +- *(1) FileScan parquet [first_name#8537,last_name#8538,income#8539] Batched: true, Format: Parquet, Location: InMemoryFileIndex[hdfs://dumbo/user/sr5388/people_small.parquet], PartitionFilters: [], PushedFilters: [IsNotNull(first_name), IsNotNull(income), EqualTo(first_name,Anna), GreaterThanOrEqual(income,70..., ReadSchema: struct<first_name:string,last_name:string,income:float>\n",
    "```\n",
    "\n",
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "  <strong>(Hint)</strong> Here we see three different columns being used in FileScan. To optimize most effectively, we therefore want to sort all three columns. This helps maximize the number of row groups we can skip! Additionally, sorted data usually compresses much better, thus taking less space/time to read. For secondary column sorting, sortWithinPartitions can help ensure you don't create excess partitions.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-warning\" role=\"alert\">\n",
    "  <strong>(Warning)</strong> Sorting has an up-front cost that and may not be useful in all applications. In some cases it may result in excess partitions/tiny files, negatively impacting read times.\n",
    "</div>\n",
    "\n",
    "*But... you implemented something for us to just automate everything right?*\n",
    "\n",
    "1. Haha - this time you only get basic functionality!\n",
    "2. There are really just two steps: choosing transformations and writing parquet files\n",
    "3. The [transform helper function](https://github.com/sroy2/parquet_optimization/blob/master/bench.py#L170-L252) in bench.py is your friend, transformations are applied **IN THE ORDER** they appear in the list! This can make a difference.\n",
    "\n",
    "```python\n",
    "#From bench.transform(spark, t_list, name, benchmark_cmd=None, runtype=False)\n",
    "    t_list : list of tuples\n",
    "        list of transformations to apply:\n",
    "        ('query','sort','value')\n",
    "        ('query','repartition','value')\n",
    "        \n",
    "        where query = 'pq_avg_income'|0, 'pq_max_income'|1, 'pq_anna'|2\n",
    "        \n",
    "    name : string\n",
    "        string appended to base filename when adding to hdfs\n",
    "        f'people_small.parquet' -> f'{query}_people_small_{name}.parquet'\n",
    "```\n",
    "\n",
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "  <strong>(Hint)</strong> I recommend the following workflow:\n",
    "</div>\n",
    "\n",
    "1. Start with ONE query.\n",
    "2. Decide what you want to tweak for that query.\n",
    "3. Build the transformation list, you can address different files with different strategies.\n",
    "4. Run the transform.\n",
    "5. Benchmark the new parquet files.\n",
    "\n",
    "```python\n",
    ">>> t_list = [('pq_avg_income', 'sort', 'zipcode'),\n",
    "              (1, 'sort', 'zipcode'),\n",
    "              (2, 'sort', 'zipcode, income, first_name')]\n",
    ">>> name = \"q1_sort\"\n",
    ">>> test = \"bench.benchmark(spark, 25, queries.pq_anna, 'hdfs:/user/\"+bench.whoami()+\"/pq_anna_people_small_\"+name+\".parquet')\"\n",
    ">>> bench.transform(spark, t_list, name)\n",
    ">>> bench.csv_table(spark, test, 'pq')\n",
    "query,file,trials,minimum,median,maximum                                        \n",
    "pq_anna,people_small,25,0.068,0.078,0.195\n",
    "pq_anna,people_medium,25,1.133,1.412,3.168\n",
    "pq_anna,people_large,25,1.081,1.607,2.743\n",
    "```\n",
    "\n",
    "<div class=\"alert alert-warning\" role=\"alert\">\n",
    "    You can automatically chain together transform and csv_table by providing the benchmark_cmd to transform directly. However, I was lazy and didn't implement full error checking to make sure you couldn't hurt yourself. You could end up running 3 batteries of tests on 9 different parquet files if you aren't careful! <br> <br>\n",
    "    bench.transform(spark=spark, t_list=t_list, name='q1_sort', benchmark_cmd=test, runtype='pq')\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "t_list = [('pq_avg_income', 'sort', 'zipcode'),\n",
    "          (1, 'sort', 'zipcode'),\n",
    "          (2, 'sort', 'zipcode, income, first_name')]\n",
    "name = \"q1_sort\"\n",
    "test = \"bench.benchmark(spark, 25, queries.pq_anna, 'hdfs:/user/\"+bench.whoami()+\"/pq_anna_people_small_\"+name+\".parquet')\"\n",
    "\n",
    "# Unchained Example\n",
    "bench.transform(spark, t_list, name)\n",
    "bench.csv_table(spark, test, 'pq')\n",
    "\n",
    "# Chained Example\n",
    "#bench.transform(spark, t_list, name, test, 'pq')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partitioning - [documentation](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?#pyspark.sql.DataFrame.repartition)\n",
    "\n",
    "In general, more partitions allow work to be distributed among more workers, but fewer partitions allow work to be done in larger chunks (and often quicker).  For this lab, we get to play around to figure out what works.\n",
    "\n",
    "<div class=\"alert alert-warning\" role=\"alert\">\n",
    "    If you are increasing the number of partitions use repartition() (performs full shuffle) <br>\n",
    "    If you are decreasing the number of partitions use coalesce() (minimizes shuffles) <br> <br>\n",
    "    Our original file was 1 partition, so we will always use repartition.\n",
    "</div>\n",
    "\n",
    "### Method 1 - (slow)\n",
    "\n",
    "Explicitly setting the partition columns when writing out to parquet is a slower, but arguably more robust method of partitioning. I didn't include this as a method you can call in transform... call me lazy. This will make an individual file for every unique value in the column you partitionBy. Recall having many small files is often poor for optimization. To test it out, try something akin to the following:\n",
    "\n",
    "```python\n",
    "file = 'hdfs:/user/'+bench.whoami()+'/people_small'\n",
    "df = spark.read.parquet(file+'.parquet')\n",
    "\n",
    "# Parition Example\n",
    "df.write.partitionBy(\"zipcode\").parquet(file+'_partition.parquet')\n",
    "```\n",
    "\n",
    "\n",
    "### Method 2 - (fast)\n",
    "\n",
    "We are just going to do the same thing we did with sorting above.\n",
    "\n",
    "```python\n",
    "# Read\n",
    "file = 'hdfs:/user/'+bench.whoami()+'/people_small'\n",
    "df = spark.read.parquet(file+'.parquet')\n",
    "\n",
    "# Parition Examples:\n",
    "df.repartition(10)\n",
    "df.repartition('zipcode')\n",
    "df.repartition('income', 'zipcode')\n",
    "df.repartition(10, 'income') #<--- my script can't handle\n",
    "\n",
    "# Write\n",
    "df.write.parquet(file+'_partition.parquet')\n",
    "```\n",
    "\n",
    "<div class=\"alert alert-danger\" role=\"alert\">\n",
    "  <strong>(Danger)</strong> repartition.() seems to have issues processing unpacked arguments. If you want to use integer values and colum names together the transform function will probably break.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-warning\" role=\"alert\">\n",
    "    <strong>(Warning)</strong> Depending on the parameters you test, files are loaded, or tasks queued up - the transform job may run out of memory and fail.  If so, try deleting any partial parquets where it failed from hdfs before trying again.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "# Method 1 - VERY SLOW!!!\n",
    "for x in ['people_small', 'people_medium', 'people_large']:\n",
    "    file = 'hdfs:/user/'+bench.whoami()+'/'+x\n",
    "    df = spark.read.parquet(file+'.parquet')\n",
    "    df.write.partitionBy(\"zipcode\").parquet(file+'_partition.parquet')\n",
    "\n",
    "# Method 2 - Slightly faster...\n",
    "t_list = [('pq_avg_income', 'repartition', 10),\n",
    "          (1, 'repartition', 'zipcode'),\n",
    "          (2, 'repartition', 'zipcode, income')]\n",
    "name = \"q2_partition\"\n",
    "test = \"bench.benchmark(spark, 25, queries.pq_anna, 'hdfs:/user/\"+bench.whoami()+\"/pq_anna_people_small_\"+name+\".parquet')\"\n",
    "\n",
    "# Unchained Example\n",
    "bench.transform(spark, t_list, name)\n",
    "bench.csv_table(spark, test, 'pq')\n",
    "\n",
    "# Chained Example\n",
    "#bench.transform(spark, t_list, name, test, 'pq')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change the HDFS replication factor\n",
    "\n",
    "The replication factor is a property that can be set in the HDFS configuration file that will allow you to adjust the global replication factor for the entire cluster. For each block stored in HDFS, there will be `n â€“ 1` duplicated blocks distributed across the cluster. For example, if the replication factor was set to `3` (default value in HDFS) there would be one original block and two replicas. [source](https://princetonits.com/how-to-configure-replication-factor-and-block-size-for-hdfs/)\n",
    "\n",
    "According to [cloudera community support](https://community.cloudera.com/t5/Support-Questions/Best-practices-between-size-block-size-file-and-replication/td-p/137168), the following are some best practices:\n",
    "1. 128mb block\n",
    "2. larger files are better\n",
    "3. minimum factor of 3 (default)\n",
    "\n",
    "So what are our settings?\n",
    "\n",
    "<div class=\"alert alert-warning\" role=\"alert\">\n",
    "    The following should be ran on your dumbo terminal, not in pyspark.\n",
    "</div>\n",
    "\n",
    "```sh\n",
    "-bash-4.1$ hdfs getconf -confKey dfs.blocksize\n",
    "134217728\n",
    "\n",
    "-bash-4.1$ hfs -ls people_small.parquet\n",
    "Found 2 items\n",
    "-rw-r--r--+  3 sr5388 users          0 2020-04-07 01:55 people_small.parquet/_SUCCESS\n",
    "-rw-r--r--+  3 sr5388 users      65497 2020-04-07 01:55 people_small.parquet/part-00000-522be778-8e49-4a4d-8c60-4930f817bda7-c000.snappy.parquet\n",
    "\n",
    "-bash-4.1$ hadoop fs -stat %o 'hdfs:/user/sr5388/people_small.parquet/part-00000-522be778-8e49-4a4d-8c60-4930f817bda7-c000.snappy.parquet'\n",
    "134217728\n",
    "\n",
    "-bash-4.1$ hdfs dfs -setrep -R 8 people_small.parquet/\n",
    "Replication 8 set: people_small.parquet/_SUCCESS\n",
    "Replication 8 set: people_small.parquet/part-00000-522be778-8e49-4a4d-8c60-4930f817bda7-c000.snappy.parquet\n",
    "\n",
    "-bash-4.1$ hfs -ls people_small.parquet\n",
    "Found 2 items\n",
    "-rw-r--r--+  8 sr5388 users          0 2020-04-07 01:55 people_small.parquet/_SUCCESS\n",
    "-rw-r--r--+  8 sr5388 users      65497 2020-04-07 01:55 people_small.parquet/part-00000-522be778-8e49-4a4d-8c60-4930f817bda7-c000.snappy.parquet\n",
    "```\n",
    "\n",
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "  <strong>(Hint)</strong> After changing the replication factor, try reloding the data and re-running some of the queries you used before! <br> <br>\n",
    "  Try setting it above and below the default (3).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "hdfs dfs -setrep -R 8 people_small.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"advanced-parquet\"></a>\n",
    "## What if we want to inspect a parquet file?\n",
    "\n",
    "<div class=\"alert alert-warning\" role=\"alert\">\n",
    "    Reading parquet files directly doesn't provide much information, so it's best to use <a href=\"https://github.com/apache/parquet-mr/tree/master/parquet-tools\">parquet-tools</a>. Unless you know the specific datanode your data resides on (ref: hdfs fsck ), it's easiest to download a local copy of the parquet for analysis.\n",
    "</div>\n",
    "\n",
    "```sh\n",
    "-bash-4.1$ hfs -copyToLocal people_small.parquet .\n",
    "-bash-4.1$ ls people_small.parquet/ | wc -l\n",
    "2\n",
    "\n",
    "-bash-4.1$ parquet-tools cat -j people_small.parquet | head -2\n",
    "{\"first_name\":\"Dennis\",\"last_name\":\"Snyder\",\"income\":55668.0,\"zipcode\":66663}\n",
    "{\"first_name\":\"Ana\",\"last_name\":\"Erickson\",\"income\":75416.0,\"zipcode\":89900}\n",
    "\n",
    "-bash-4.1$ parquet-tools meta --debug people_small.parquet/part-00000-522be778-8e49-4a4d-8c60-4930f817bda7-c000.snappy.parquet | cat\n",
    "creator:     parquet-mr version 1.10.0 (build 031a6654009e3b82020012a18434c582bd74c73a) \n",
    "extra:       org.apache.spark.sql.parquet.row.metadata = {\"type\":\"struct\",\"fields\":[{\"name\":\"first_name\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"last_name\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"income\",\"type\":\"float\",\"nullable\":true,\"metadata\":{}},{\"name\":\"zipcode\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}}]} \n",
    "\n",
    "file schema: spark_schema \n",
    "---------------------------------------------------------------------------------------\n",
    "first_name:  OPTIONAL BINARY O:UTF8 R:0 D:1\n",
    "last_name:   OPTIONAL BINARY O:UTF8 R:0 D:1\n",
    "income:      OPTIONAL FLOAT R:0 D:1\n",
    "zipcode:     OPTIONAL INT32 R:0 D:1\n",
    "\n",
    "row group 1: RC:5000 TS:69519 \n",
    "---------------------------------------------------------------------------------------\n",
    "first_name:   BINARY SNAPPY DO:0 FPO:4 SZ:11027/13109/1.19 VC:5000 ENC:BIT_PACKED,RLE,PLAIN_DICTIONARY\n",
    "last_name:    BINARY SNAPPY DO:0 FPO:11031 SZ:13494/16296/1.21 VC:5000 ENC:BIT_PACKED,RLE,PLAIN_DICTIONARY\n",
    "income:       FLOAT SNAPPY DO:0 FPO:24525 SZ:20063/20057/1.00 VC:5000 ENC:BIT_PACKED,RLE,PLAIN\n",
    "zipcode:      INT32 SNAPPY DO:0 FPO:44588 SZ:20063/20057/1.00 VC:5000 ENC:BIT_PACKED,RLE,PLAIN\n",
    "-bash-4.1$  \n",
    "```\n",
    "\n",
    "### Meta Legend\n",
    "\n",
    "#### Row Group Totals\n",
    "\n",
    "Acronym | Definition\n",
    "--------|-----------\n",
    "RC | Row Count\n",
    "TS | Total Byte Size\n",
    "\n",
    "#### Row Group Column Details\n",
    "\n",
    "Acronym | Definition\n",
    "--------|-----------\n",
    "DO | Dictionary Page Offset\n",
    "FPO | First Data Page Offset\n",
    "SZ:{x}/{y}/{z} | Size in bytes. x = Compressed total, y = uncompressed total, z = y:x ratio\n",
    "VC | Value Count\n",
    "RLE | Run-Length Encoding\n",
    "\n",
    "\n",
    "## Did sorting do anything?\n",
    "\n",
    "```sh\n",
    "-bash-4.1$ hfs -copyToLocal people_small_q1_sort.parquet .\n",
    "-bash-4.1$ ls people_small_q1_sort.parquet/ | wc -l\n",
    "201\n",
    "\n",
    "-bash-4.1$ parquet-tools cat -j people_small_q1_sort.parquet | head -2\n",
    "{\"first_name\":\"Damon\",\"last_name\":\"Oliver\",\"income\":75396.0,\"zipcode\":98537}\n",
    "{\"first_name\":\"Penny\",\"last_name\":\"Ponce\",\"income\":83608.0,\"zipcode\":98556}\n",
    "\n",
    "-bash-4.1$ parquet-tools meta --debug people_small_q1_sort.parquet/part-00000-ab36572a-b831-4b24-a27a-b7315acb853d-c000.snappy.parquet | cat\n",
    "creator:     parquet-mr version 1.10.0 (build 031a6654009e3b82020012a18434c582bd74c73a) \n",
    "extra:       org.apache.spark.sql.parquet.row.metadata = {\"type\":\"struct\",\"fields\":[{\"name\":\"first_name\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"last_name\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"income\",\"type\":\"float\",\"nullable\":true,\"metadata\":{}},{\"name\":\"zipcode\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}}]} \n",
    "\n",
    "file schema: spark_schema \n",
    "---------------------------------------------------------------------------------------\n",
    "first_name:  OPTIONAL BINARY O:UTF8 R:0 D:1\n",
    "last_name:   OPTIONAL BINARY O:UTF8 R:0 D:1\n",
    "income:      OPTIONAL FLOAT R:0 D:1\n",
    "zipcode:     OPTIONAL INT32 R:0 D:1\n",
    "\n",
    "row group 1: RC:25 TS:887 \n",
    "---------------------------------------------------------------------------------------\n",
    "first_name:   BINARY SNAPPY DO:0 FPO:4 SZ:273/293/1.07 VC:25 ENC:RLE,PLAIN,BIT_PACKED\n",
    "last_name:    BINARY SNAPPY DO:0 FPO:277 SZ:262/288/1.10 VC:25 ENC:RLE,PLAIN,BIT_PACKED\n",
    "income:       FLOAT SNAPPY DO:0 FPO:539 SZ:156/153/0.98 VC:25 ENC:RLE,PLAIN,BIT_PACKED\n",
    "zipcode:      INT32 SNAPPY DO:0 FPO:695 SZ:156/153/0.98 VC:25 ENC:RLE,PLAIN,BIT_PACKED\n",
    "-bash-4.1$  \n",
    "```\n",
    "\n",
    "\n",
    "## Great - but I want to actually mess up my Hive!\n",
    "\n",
    "<img src=\"img/11.jpg\" width=\"500px\"/>\n",
    "\n",
    "Ok, you're on your own here - I can't teach you enough to keep you safe. I will show you some places to start looking though.\n",
    "\n",
    "```python \n",
    ">>> from pyspark import SparkContext, SQLContext, HiveContext\n",
    ">>> hiveContext = HiveContext(spark.sparkContext)\n",
    "\n",
    ">>> spark.sparkContext.getConf().getAll()\n",
    "[('spark.dynamicAllocation.enabled', 'false'), ('spark.eventLog.enabled', 'true'), ('spark.driver.host', 'login-1-1.local'), ('spark.executor.extraLibraryPath', '/opt/cloudera/parcels/CDH-5.15.2-1.cdh5.15.2.p0.3/lib/hadoop/lib/native'), ('spark.ui.proxyBase', '/proxy/application_1579315286937_37050'), ('spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_HOSTS', 'babar.es.its.nyu.edu,compute-1-8.local'), ('spark.eventLog.dir', 'hdfs://dumbo/user/spark/spark2ApplicationHistory'), ('spark.driver.appUIAddress', 'http://login-1-1.local:4051'), ('spark.dynamicAllocation.executorIdleTimeout', '60'), ('spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.RM_HA_URLS', 'babar.es.its.nyu.edu:8088,compute-1-8.local:8088'), ('spark.yarn.historyServer.address', 'http://babar.es.its.nyu.edu:18089'), ('spark.serializer.objectStreamReset', '100'), ('spark.submit.deployMode', 'client'), ('spark.ui.filters', 'org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter'), ('spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_URI_BASES', 'http://babar.es.its.nyu.edu:8088/proxy/application_1579315286937_37050,http://compute-1-8.local:8088/proxy/application_1579315286937_37050'), ('spark.executor.memory', '2g'), ('spark.driver.port', '41730'), ('spark.shuffle.service.enabled', 'true'), ('spark.executor.id', 'driver'), ('spark.dynamicAllocation.schedulerBacklogTimeout', '1'), ('spark.yarn.am.extraLibraryPath', '/opt/cloudera/parcels/CDH-5.15.2-1.cdh5.15.2.p0.3/lib/hadoop/lib/native'), ('spark.shuffle.service.port', '7337'), ('spark.app.name', 'PySparkShell'), ('spark.master', 'yarn'), ('spark.app.id', 'application_1579315286937_37050'), ('spark.sql.warehouse.dir', '/user/hive/warehouse'), ('spark.sql.catalogImplementation', 'hive'), ('spark.rdd.compress', 'True'), ('spark.executorEnv.PYTHONPATH', '/share/apps/spark/spark-2.4.0-bin-hadoop2.6/python/lib/py4j-0.10.7-src.zip:/share/apps/spark/spark-2.4.0-bin-hadoop2.6/python/:/share/apps/spark/spark-2.4.0-bin-hadoop2.6/python:/share/apps/spark/spark-2.4.0-bin-hadoop2.6/python/lib/py4j-0.10.7-src.zip<CPS>{{PWD}}/pyspark.zip<CPS>{{PWD}}/py4j-0.10.7-src.zip'), ('spark.dynamicAllocation.minExecutors', '0'), ('spark.driver.extraLibraryPath', '/opt/cloudera/parcels/CDH-5.15.2-1.cdh5.15.2.p0.3/lib/hadoop/lib/native'), ('spark.yarn.isPython', 'true'), ('spark.ui.showConsoleProgress', 'true'), ('spark.port.maxRetries', '100')]\n",
    "```\n",
    "\n",
    "<div class=\"alert alert-danger\" role=\"alert\">\n",
    "  <strong>(Danger!)</strong>\n",
    "  Changing values may have unintended consequences!\n",
    "</div>\n",
    "\n",
    "**Good parameters begin with:** \n",
    "> parquet.filter.statistics.enabled <br>\n",
    "> parquet.filter.dictionary.enabled <br>\n",
    "> parquet.dictionary.page.size <br>\n",
    "> parquet.block.size <br>\n",
    "> spark.sql.parquet.mergeSchema <br>\n",
    "> spark.sql.parquet.filterPushdown <br>\n",
    "> spark.sql.hive.metastorePartitionPruning <br>\n",
    "> spark.sql.hive.convertMetastoreParquet <br>\n",
    "> spark.sql.hive.convertMetastoreParquet.mergeSchema <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"end\"></a>\n",
    "# Just straight code for the copy pasters out there...\n",
    "\n",
    "<div class=\"alert alert-danger\" role=\"alert\">\n",
    "  <strong>(Warning)</strong>\n",
    "  You MUST finish the SQL queries in queries.py before moving on to the code below.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-warning\" role=\"alert\">\n",
    "    <strong>(Warning)</strong> These are pretty bad t_list choices. Think about what makes sense to you and update accordingly.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "  <strong>(Hint)</strong> \n",
    "  Load the below modules and make sure you are in the lab4 directory before calling pyspark and proceeding!\n",
    "</div>\n",
    "\n",
    "```sh\n",
    "cd `ls -d ~/* | grep \"lab4\"`\n",
    "module load python/gnu/3.6.5\n",
    "module load spark/2.4.0\n",
    "pyspark\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "import queries\n",
    "import bench\n",
    "\n",
    "## Problems 1 & 2\n",
    "bench.copy(spark)\n",
    "test = \"bench.benchmark(spark, 25, queries.pq_anna, 'hdfs:/user/\"+bench.whoami()+\"/people_small.parquet')\"\n",
    "bench.csv_table(spark, test, 'all both')\n",
    "\n",
    "## Problem 3\n",
    "f_test = lambda : f\"bench.benchmark(spark, 25, queries.pq_anna, 'hdfs:/user/{bench.whoami()}/pq_anna_people_small_{name}.parquet')\"\n",
    "\n",
    "# Sort\n",
    "t_list = [(0, 'sort', 'zipcode'),\n",
    "          (1, 'sort', 'zipcode'),\n",
    "          (2, 'sort', 'zipcode, income, first_name')]\n",
    "name = \"q1_sort\"\n",
    "bench.transform(spark, t_list, name, f_test(), 'pq')\n",
    "\n",
    "# Partition\n",
    "t_list = [(0, 'repartition', 10),\n",
    "          (1, 'repartition', 'zipcode'),\n",
    "          (2, 'repartition', 'zipcode, income')]\n",
    "name = \"q2_partition\"\n",
    "bench.transform(spark, t_list, name, f_test(), 'pq')\n",
    "\n",
    "# Sort then Partition\n",
    "# (you will run out of memory with these settings!)\n",
    "t_list = [(0, 'sort', 'zipcode'),\n",
    "          (1, 'sort', 'zipcode'),\n",
    "          (2, 'sort', 'zipcode, income, first_name'),\n",
    "          (0, 'repartition', 10),\n",
    "          (1, 'repartition', 'zipcode'),\n",
    "          (2, 'repartition', 'zipcode')]\n",
    "name = \"q3_sort_then_partition\"\n",
    "bench.transform(spark, t_list, name, f_test(), 'pq')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "# Run direction on dumbo, not in pyspark\n",
    "\n",
    "hdfs dfs -expunge\n",
    "hdfs dfs -setrep -R 1 \"./pq*\"\n",
    "# rerun tests above, no transform required\n",
    "# \n",
    "\n",
    "hdfs dfs -setrep -R 6 \"./pq*\"\n",
    "# rerun tests above, no transform required\n",
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
